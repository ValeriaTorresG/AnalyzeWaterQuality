{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505eea5e-0351-4b18-8b96-3143b4559da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ee\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 250\n",
    "\n",
    "from patch_classifier_module import PatchClassifier\n",
    "\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa70731-f584-48f4-b361-e4470b8e6d6f",
   "metadata": {},
   "source": [
    "## Input-Channel Construction\n",
    "\n",
    "For each scene (Chesapeake and CGSM), compute (B2, B3, B4, B8) and two robust indices:\n",
    "\n",
    "- NDTI = (B4 – B3) / (B4 + B3)\n",
    "\n",
    "- NDWI = (B3 – B8) / (B3 + B8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f91ed5",
   "metadata": {},
   "source": [
    "### Ciénaga Grande Santa Marta (CGSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207448e",
   "metadata": {},
   "source": [
    "#### 1. Initial paramaters\n",
    "\n",
    "- patch_size: the side length (in pixels) of each square patch we’ll extract around a station\n",
    "\n",
    "- scale: ground resolution of Sentinel-2 in meters per pixel\n",
    "\n",
    "- half_m / half_deg: how many meters (and degrees) to go out from the station coordinate to build a square of size patch_size\n",
    "\n",
    "- bands: the Sentinel-2 bands (and two indices) we’ll pull for each patch\n",
    "\n",
    "- window_days: temporal window around each in-situ sample date\n",
    "\n",
    "- cloud_thresh: maximum allowed cloud cover when filtering images\n",
    "\n",
    "- debug_level: controls printing progress messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2333e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 128\n",
    "scale = 10\n",
    "half_m = (patch_size // 2) * scale\n",
    "meters_per_deg = 111320.0\n",
    "half_deg = half_m / meters_per_deg\n",
    "bands = ['B2','B3','B4','B8','NDTI','NDWI']\n",
    "window_days = 3\n",
    "cloud_thresh = 50\n",
    "debug_level = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc42add5",
   "metadata": {},
   "source": [
    "#### 2. Z-score normalization function\n",
    "\n",
    "- Takes a 3D array with shape (C, H, W)\n",
    "\n",
    "- Computes per-band mean and std, then standardizes so each band has mean 0 and unit variance\n",
    "\n",
    "- Prevents divide-by-zero with a small epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize each band of (C,H,W) array.\"\"\"\n",
    "    mean = x.mean(axis=(1,2), keepdims=True)\n",
    "    std = x.std(axis=(1,2), keepdims=True) + 1e-8\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89b248",
   "metadata": {},
   "source": [
    "#### 3. Load and filter the CSV of in-situ data\n",
    "\n",
    "1. Read the CSV, parsing the muestreo column as dates\n",
    "\n",
    "2. Select only the latitude, longitude, station name, sample date, and SST measurement\n",
    "\n",
    "3. Print a sample of station names if debugging is on.\n",
    "\n",
    "4. Keep only the four stations located in the Ciénaga Grande de Santa Marta—and exit with an error if none match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84735299",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/icam.csv',\n",
    "                parse_dates=['muestreo'],\n",
    "                low_memory=False)\n",
    "\n",
    "df = df[['latitud','longitud','estacion','muestreo','sst']].dropna().copy()\n",
    "\n",
    "if debug_level:\n",
    "    print(\"Available stations (sample):\", df['estacion'].unique()[:10])\n",
    "\n",
    "estaciones_cienaga = ['F. Costa Verde',\n",
    "                      'F. La Barra',\n",
    "                      'F. Sevilla',\n",
    "                      'F. Palma Sola',]\n",
    "\n",
    "df = df[df['estacion'].isin(estaciones_cienaga)].reset_index(drop=True)\n",
    "if df.empty:\n",
    "    print(\"ERROR: No stations match filter. Check station names.\" )\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c3e46f",
   "metadata": {},
   "source": [
    "#### 4. Initialize the Earth Engine collection\n",
    "\n",
    "Build a base Sentinel-2 collection:\n",
    "\n",
    "- Filter out scenes with > cloud_thresh% cloud cover\n",
    "\n",
    "- Keep only the four visible bands we need\n",
    "\n",
    "Prepare empty lists for the extracted patches and corresponding labels, plus a counter for skipped rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7eb5ad-7205-4a7f-9e51-2125219fa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = (\n",
    "    ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "      .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', cloud_thresh))\n",
    "      .select(['B2','B3','B4','B8'])\n",
    ")\n",
    "patches, labels = [], []\n",
    "skipped = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a8a63",
   "metadata": {},
   "source": [
    "#### 5. Iterate over each in-situ sample and extract a patch\n",
    "\n",
    "1. Loop over each valid row of in-situ measurements\n",
    "\n",
    "2. Define a ±window_days window around the sample date\n",
    "\n",
    "3. Define a square region of size patch_size×patch_size around the station coordinates\n",
    "\n",
    "4. Filter the Sentinel-2 collection by that region and date window; if no images, skip\n",
    "\n",
    "5. Compute the band-wise median (base) and two indices (NDTI, NDWI)\n",
    "\n",
    "6. Reproject to ensure consistent pixel scale\n",
    "\n",
    "7. Sample all bands into a Python dict, fill missing data with zeros\n",
    "\n",
    "8. Stack into a NumPy array of shape (6, 128, 128)\n",
    "\n",
    "9. Normalize each band, then append to the list with its corresponding SST label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    lat, lon    = float(row['latitud']), float(row['longitud'])\n",
    "    date        = row['muestreo']\n",
    "    sst_val     = float(row['sst'])\n",
    "    station     = row['estacion']\n",
    "\n",
    "    start = (date - timedelta(days=window_days)).strftime('%Y-%m-%d')\n",
    "    end   = (date + timedelta(days=window_days)).strftime('%Y-%m-%d')\n",
    "\n",
    "    region = ee.Geometry.Rectangle([\n",
    "        lon - half_deg, lat - half_deg,\n",
    "        lon + half_deg, lat + half_deg\n",
    "    ])\n",
    "\n",
    "    sentinel = collection.filterBounds(region).filterDate(start, end)\n",
    "    try:\n",
    "        count = sentinel.size().getInfo()\n",
    "    except Exception as e:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    if debug_level:\n",
    "        print(f\"Row {i}: Station={station}, Date={date.date()}, Images={count}\")\n",
    "    if count == 0:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    base = sentinel.median()\n",
    "    ndti = base.normalizedDifference(['B4','B3']).rename('NDTI')\n",
    "    ndwi = base.normalizedDifference(['B3','B8']).rename('NDWI')\n",
    "    comp = base.addBands([ndti, ndwi]).unmask(0)\n",
    "    reproj = comp.reproject(comp.projection().atScale(scale))\n",
    "\n",
    "    try:\n",
    "        data_dict = reproj.sampleRectangle(region=region, defaultValue=0) \\\n",
    "                          .toDictionary().getInfo()\n",
    "    except Exception as e:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    arrs = []\n",
    "    for b in bands:\n",
    "        mat = np.array(data_dict.get(b, np.zeros((patch_size, patch_size))))\n",
    "        if mat.shape != (patch_size, patch_size):\n",
    "            mat = np.resize(mat, (patch_size, patch_size))\n",
    "        arrs.append(mat)\n",
    "    patch = np.stack(arrs, axis=0)\n",
    "\n",
    "    patch = zscore_normalize(patch)\n",
    "    patches.append(patch)\n",
    "    labels.append(sst_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff405ab",
   "metadata": {},
   "source": [
    "#### 6. Save the extracted dataset\n",
    "\n",
    "- Verify that at least one patch was extracted, otherwise exit with error\n",
    "\n",
    "- Stack all patches into a single array of shape (N, C, H, W) and labels into (N,)\n",
    "\n",
    "- Save both to .npy files for subsequent model training or inference\n",
    "\n",
    "- Report how many patches were created versus skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not patches:\n",
    "    print(\"ERROR: No patches extracted. Adjust filters or check data.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "X_cgs = np.stack(patches, axis=0)\n",
    "y_cgs = np.array(labels)\n",
    "\n",
    "np.save('./data/cgs_patches.npy', X_cgs)\n",
    "np.save('./data/cgs_labels.npy', y_cgs)\n",
    "print(f\"Extracted {len(patches)} patches, skipped {skipped} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a4a31",
   "metadata": {},
   "source": [
    "### Chesapeake Bay (CB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef2792",
   "metadata": {},
   "source": [
    "#### 1. Initial parameters\n",
    "\n",
    "- patch_size: width/height in pixels of each square patch.\n",
    "\n",
    "- scale: meters on the ground represented by one pixel.\n",
    "\n",
    "- half_deg: how many degrees of latitude/longitude correspond to half the patch’s ground footprint.\n",
    "\n",
    "- THRESHOLD: converts a continuous measurement into a binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 128\n",
    "scale = 10\n",
    "half_m = (patch_size // 2) * scale\n",
    "meters_per_deg = 111320.0\n",
    "half_deg = half_m / meters_per_deg\n",
    "THRESHOLD = 25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743ee8d",
   "metadata": {},
   "source": [
    "#### 2. Z-score normalization function\n",
    "\n",
    "- Takes a 3D array with shape (C, H, W)\n",
    "\n",
    "- Computes per-band mean and std, then standardizes so each band has mean 0 and unit variance\n",
    "\n",
    "- Prevents divide-by-zero with a small epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize(x: np.ndarray, eps: float = 1e-8) -> np.ndarray:\n",
    "    if x.ndim == 3:\n",
    "        # normaliza un solo parche\n",
    "        mean = x.mean(axis=(1,2), keepdims=True)\n",
    "        std  = x.std(axis=(1,2), keepdims=True) + eps\n",
    "    elif x.ndim == 4:\n",
    "        # normaliza todo el batch de parches\n",
    "        mean = x.mean(axis=(2,3), keepdims=True)\n",
    "        std  = x.std(axis=(2,3), keepdims=True) + eps\n",
    "    else:\n",
    "        raise ValueError(f\"zscore_normalize espera ndim 3 o 4, got {x.ndim}\")\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68f8d3",
   "metadata": {},
   "source": [
    "#### 3. Load & Filter the Chesapeake In-Situ Data\n",
    "\n",
    "1. Read the CSV containing many water-quality measurements around Chesapeake Bay\n",
    "\n",
    "2. Select only rows where the parameter is total suspended solids ('TSS')\n",
    "\n",
    "3. Extract the relevant columns—timestamp, coordinates, and numeric value—dropping any exact duplicates to get a clean list of sampling events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3788e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cb_in_situ.csv',\n",
    "                 parse_dates=['SampleDate'],\n",
    "                 low_memory=False)\n",
    "\n",
    "df_turb = df[df['Parameter'] == 'TSS'].copy()\n",
    "samples = (\n",
    "    df_turb[['SampleDate', 'Latitude', 'Longitude', 'MeasureValue']]\n",
    "      .drop_duplicates()\n",
    "      .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7cfeb",
   "metadata": {},
   "source": [
    "#### 4. Prepare Containers & Sentinel-2 Collection\n",
    "\n",
    "- patches, masks: empty lists that will hold the image data and their binary labels\n",
    "\n",
    "- collection: a filtered Earth Engine image collection selecting only scenes with ≤ 30 % clouds and the four visible bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches, masks = [], []\n",
    "collection = (\n",
    "    ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "      .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n",
    "      .select(['B2','B3','B4','B8'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653bbcec",
   "metadata": {},
   "source": [
    "#### 5. Iterate Over Each Sample, Extract & Label Patches\n",
    "\n",
    "1. Define a one-day window around each sample and a square region around its coordinates\n",
    "\n",
    "2. Filter the Sentinel-2 data both in space and time\n",
    "\n",
    "3. Build a median composite image and compute two water-quality indices (NDTI, NDWI)\n",
    "\n",
    "4. Reproject to ensure consistent pixel size\n",
    "\n",
    "5. Sample the pixel grid into memory\n",
    "\n",
    "6. Stack the six bands into a 3D array\n",
    "\n",
    "7. Apply Z-score normalization\n",
    "\n",
    "8. Create a uniform binary mask (0 or 1) based on the turbidity threshold, same size as the patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dee3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in samples.iterrows():\n",
    "    sd    = row['SampleDate']\n",
    "    label = float(row['MeasureValue'])\n",
    "\n",
    "    start = sd.strftime('%Y-%m-%d')\n",
    "    end   = (sd + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "    lon, lat = float(row['Longitude']), float(row['Latitude'])\n",
    "    region = ee.Geometry.Rectangle([\n",
    "        lon - half_deg, lat - half_deg,\n",
    "        lon + half_deg, lat + half_deg\n",
    "    ])\n",
    "\n",
    "    sentinel = collection.filterBounds(region).filterDate(start, end)\n",
    "    count = sentinel.size().getInfo()\n",
    "    if count == 0:\n",
    "        continue\n",
    "\n",
    "    base = sentinel.median()\n",
    "    ndti = base.normalizedDifference(['B4','B3']).rename('NDTI')\n",
    "    ndwi = base.normalizedDifference(['B3','B8']).rename('NDWI')\n",
    "    comp = base.addBands([ndti, ndwi]).unmask(0)\n",
    "    reproj = comp.reproject(comp.projection().atScale(scale))\n",
    "\n",
    "    data_dict = reproj.sampleRectangle(\n",
    "        region=region, defaultValue=0\n",
    "    ).toDictionary().getInfo()\n",
    "\n",
    "    bands = ['B2','B3','B4','B8','NDTI','NDWI']\n",
    "    arrays = []\n",
    "    for b in bands:\n",
    "        arr = np.array(data_dict.get(b, np.zeros((patch_size, patch_size))))\n",
    "        if arr.shape != (patch_size, patch_size):\n",
    "            arr = np.resize(arr, (patch_size, patch_size))\n",
    "        arrays.append(arr)\n",
    "        \n",
    "    patch = np.stack(arrays, axis=0)\n",
    "\n",
    "    patches.append(zscore_normalize(patch))\n",
    "\n",
    "    binary_class = 1 if label > THRESHOLD else 0\n",
    "    mask = np.full((patch_size, patch_size), binary_class, dtype=np.uint8)\n",
    "    masks.append(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf38f66",
   "metadata": {},
   "source": [
    "#### 6. Stack & Save the Dataset\n",
    "\n",
    "- Combine all individual patches into one big array X_data of shape (N, 6, 128, 128)\n",
    "\n",
    "- Combine all masks into y_mask of shape (N, 128, 128)\n",
    "\n",
    "- Save both arrays as .npy files for use in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd02c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.stack(patches, axis=0)\n",
    "y_mask = np.stack(masks, axis=0)\n",
    "\n",
    "np.save('./data/patches_array.npy', X_data)\n",
    "np.save('./data/labels_array.npy', y_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ac93c-f5aa-457d-acdf-fac83774d070",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139e61b",
   "metadata": {},
   "source": [
    "#### 1. Hyperparameters\n",
    "\n",
    "- BATCH_SIZE: number of samples per training batch\n",
    "\n",
    "- LR: initial learning rate for the Adam optimizer\n",
    "\n",
    "- WEIGHT_DECAY: L2 regularization strength\n",
    "\n",
    "- NUM_EPOCHS: maximum epochs to train\n",
    "\n",
    "- PATIENCE: how many epochs without validation improvement before early stopping\n",
    "\n",
    "- AUG_PROB & NOISE_STD: probability and strength of data augmentations (flips, rotations, Gaussian noise)\n",
    "\n",
    "- FINETUNE_LR & FINETUNE_EPOCHS: settings for a later fine‐tuning stage (not shown here)\n",
    "\n",
    "- THRESHOLD: cutoff on sigmoid outputs to convert probabilities into binary predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510386c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "LR = 5e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "AUG_PROB = 0.5\n",
    "NOISE_STD = 0.02\n",
    "FINETUNE_LR = 1e-6\n",
    "FINETUNE_EPOCHS = 20\n",
    "THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ae3411",
   "metadata": {},
   "source": [
    "#### 2. Z-score Normalization Utility\n",
    "\n",
    "Standardizes each channel of every patch to zero mean and unit variance, preventing any one band from dominating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15088cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    mean = x.mean(axis=(2,3), keepdims=True)\n",
    "    std  = x.std(axis=(2,3), keepdims=True) + 1e-8\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14b57f",
   "metadata": {},
   "source": [
    "#### 3. Custom Dataset Class\n",
    "\n",
    "Wraps NumPy arrays into a PyTorch Dataset\n",
    "\n",
    "Optionally applies spatial augmentations and small noise on the fly for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c28e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurbidityDataset(Dataset):\n",
    "    def __init__(self, X, y, augment=False):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float().unsqueeze(1)\n",
    "        self.augment = augment\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.X[idx], self.y[idx]\n",
    "        if self.augment:\n",
    "            # random vertical flip\n",
    "            if random.random() < AUG_PROB:\n",
    "                x = torch.flip(x, dims=[1])\n",
    "            # random horizontal flip\n",
    "            if random.random() < AUG_PROB:\n",
    "                x = torch.flip(x, dims=[2])\n",
    "            # random 90° rotation\n",
    "            if random.random() < AUG_PROB:\n",
    "                k = random.choice([1,2,3])\n",
    "                x = torch.rot90(x, k, dims=[1,2])\n",
    "            # additive Gaussian noise\n",
    "            if random.random() < AUG_PROB:\n",
    "                x = x + torch.randn_like(x) * NOISE_STD\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe7df6b",
   "metadata": {},
   "source": [
    "#### 4. Load Data & Create Splits\n",
    "\n",
    "- Loads the saved patch and mask arrays\n",
    "\n",
    "- Reduces each mask to a single scalar label by sampling the center pixel\n",
    "\n",
    "- Normalizes the input patches\n",
    "\n",
    "- Splits into train/val/test, preserving class balance (stratification)\n",
    "\n",
    "- Creates DataLoaders for efficient batching and optional shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225debab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('./data/patches_array.npy')\n",
    "y_mask = np.load('./data/labels_array.npy')\n",
    "H, W = y_mask.shape[1], y_mask.shape[2]\n",
    "y = y_mask[:, H//2, W//2]\n",
    "X = zscore_normalize(X)\n",
    "\n",
    "# 10% hold-out test split, stratified\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# of the remaining 90%, 80% train and 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(TurbidityDataset(X_train, y_train, augment=True),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n",
    "val_loader   = DataLoader(TurbidityDataset(X_val,   y_val,   augment=False),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(TurbidityDataset(X_test,  y_test,  augment=False),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a491b6",
   "metadata": {},
   "source": [
    "#### 5. Model Definition\n",
    "\n",
    "DoubleConv: two Conv→BN→ReLU blocks in sequence\n",
    "\n",
    "PatchClassifier: a small CNN encoder that downsamples twice, then global pools to a 256-dim vector per patch and applies a linear layer to predict one logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.seq(x)\n",
    "\n",
    "class PatchClassifier(nn.Module):\n",
    "    def __init__(self, in_channels=6):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            DoubleConv(in_channels,64), nn.MaxPool2d(2),\n",
    "            DoubleConv(64,128),      nn.MaxPool2d(2),\n",
    "            DoubleConv(128,256),     nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(256,1)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)                    # (B,256,1,1)\n",
    "        return self.fc(x.view(x.size(0), -1))  # (B,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341fe47",
   "metadata": {},
   "source": [
    "#### 6. Training & Validation Loop\n",
    "\n",
    "1. Move model and data to GPU if available\n",
    "\n",
    "2. Compute a pos_weight to counter class imbalance\n",
    "\n",
    "3. Train for up to NUM_EPOCHS, tracking training and validation loss\n",
    "\n",
    "4. Adjust learning rate on plateau of validation loss\n",
    "\n",
    "5. Save the best checkpoint and stop early if no improvement for PATIENCE epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4638d5-c516-4347-9156-1efe066fdc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "                      else 'mps' if torch.backends.mps.is_available()\n",
    "                      else 'cpu')\n",
    "print(f'Using: {device}')\n",
    "model = PatchClassifier(in_channels=X.shape[1]).to(device)\n",
    "\n",
    "pos = (y_train==1).sum(); neg = (y_train==0).sum()\n",
    "pos_weight = torch.tensor(neg/(pos + 1e-8), device=device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,'min', factor=0.7, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e581fa-d5c3-40da-87d9-258b7bb8d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val = float('inf')\n",
    "epochs_no_imp = 10\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for ep in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        loss = criterion(model(xb), yb)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            val_loss += criterion(model(xb), yb).item() * xb.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val, epochs_no_imp = val_loss, 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_no_imp += 1\n",
    "\n",
    "    print(f\"Epoch {ep} | Train {train_loss:.4f} | Val {val_loss:.4f}\")\n",
    "    if epochs_no_imp >= PATIENCE:\n",
    "        print(\"Stopping early.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64fbe7d-3b68-4a33-9bfa-8b73f6d0fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(np.arange(1, len(train_losses)+1), train_losses, label='Train loss')\n",
    "plt.plot(np.arange(1, len(val_losses)+1), val_losses, label='Validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffc795",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5885b8c",
   "metadata": {},
   "source": [
    "#### 7. Final Evaluation on Test Set\n",
    "\n",
    "- Make predictions on the held-out test set\n",
    "\n",
    "- Threshold sigmoid outputs to generate binary labels\n",
    "\n",
    "- Calculate Accuracy, Precision, Recall, AUC, and print a Confusion Matrix to quantify final performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e20d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        probs = torch.sigmoid(model(xb)).cpu().view(-1)\n",
    "        preds = (probs > THRESHOLD).int().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.view(-1).numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "test_prec = precision_score(all_labels, all_preds)\n",
    "test_rec = recall_score(all_labels, all_preds)\n",
    "test_auc = roc_auc_score(all_labels, all_preds)\n",
    "conf_mat = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(\"Test Accuracy: \", test_acc)\n",
    "print(\"Precision: \", test_prec)\n",
    "print(\"Recall: \", test_rec)\n",
    "print(\"AUC: \", test_auc)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562569a6",
   "metadata": {},
   "source": [
    "## Fine-Tuning on CGSM Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562346e",
   "metadata": {},
   "source": [
    "- Objective: Adapt the pre-trained model (on Chesapeake) to the local Ciénaga domain\n",
    "\n",
    "- Load the saved NumPy arrays of patches (.npy) and their continuous SST labels\n",
    "\n",
    "- Apply the same Z-score normalization to match the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cgsm = np.load('./data/cgs_patches.npy')\n",
    "y_cgsm = np.load('./data/cgs_labels.npy')\n",
    "\n",
    "X_cgsm = zscore_normalize(X_cgsm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d04d2",
   "metadata": {},
   "source": [
    "#### 1. Create a Train/Hold-out Split with Graceful Fallback\n",
    "\n",
    "- Stratified split keeps the same proportion of “contaminated” vs. “clean” patches in both fine-tune and hold-out sets\n",
    "\n",
    "- Fallback: if one class has only a single example (making stratification impossible), it automatically uses a standard random split instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_ft, X_hold, y_ft, y_hold = train_test_split(\n",
    "        X_cgsm, y_cgsm,\n",
    "        test_size=0.2, random_state=1,\n",
    "        stratify=y_cgsm\n",
    "    )\n",
    "    print(\"Using stratified split\")\n",
    "except ValueError:\n",
    "    # If one class has too few samples, fall back to a simple random split\n",
    "    print(\"Stratified split failed, using random split\")\n",
    "    X_ft, X_hold, y_ft, y_hold = train_test_split(\n",
    "        X_cgsm, y_cgsm,\n",
    "        test_size=0.2, random_state=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    print(\"Using random split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86790c1f",
   "metadata": {},
   "source": [
    "#### 2. Wrap in Datasets & Loaders\n",
    "\n",
    "- dataset_ft: the fine-tuning set with augmentations turned on\n",
    "\n",
    "- dataset_hold: a small hold-out set (no augmentations) to monitor potential overfitting during fine-tuning\n",
    "\n",
    "- DataLoaders batch and shuffle the data appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ft = TurbidityDataset(X_ft,  y_ft,  augment=True)\n",
    "dataset_hold = TurbidityDataset(X_hold, y_hold, augment=False)\n",
    "loader_ft = DataLoader(dataset_ft,  batch_size=BATCH_SIZE, shuffle=True)\n",
    "loader_hold = DataLoader(dataset_hold, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a584e91",
   "metadata": {},
   "source": [
    "#### 3. Fine-Tune the Full Model\n",
    "\n",
    "- Lower learning rate (FINETUNE_LR) ensures small weight updates to preserve the previously learned features\n",
    "\n",
    "- No scheduler or early stopping here—just a fixed number of fine-tuning epochs\n",
    "\n",
    "- Print the average fine-tuning loss each epoch to verify convergence on the new domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6dcdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_f = torch.optim.Adam(model.parameters(), lr=FINETUNE_LR)\n",
    "for ep in range(1, FINETUNE_EPOCHS+1):\n",
    "    model.train()\n",
    "    loss_ft = 0\n",
    "    for xb, yb in loader_ft:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        loss = criterion(model(xb), yb)\n",
    "        optimizer_f.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_f.step()\n",
    "        loss_ft += loss.item() * xb.size(0)\n",
    "    loss_ft /= len(loader_ft.dataset)\n",
    "    print(f\"Fine-tune Epoch {ep}, Loss: {loss_ft:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bfbe3c",
   "metadata": {},
   "source": [
    "#### 4. Visual Inspection of Predictions\n",
    "\n",
    "- Select one batch from your original test set for inspection\n",
    "\n",
    "- Compute sigmoid probabilities and threshold them into binary predictions\n",
    "\n",
    "- Plot the first four patches as RGB composites (using bands B4, B3, B2)\n",
    "\n",
    "- Overlay the ground-truth (center-pixel) label vs. the model’s prediction in the title for quick visual validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b527f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "batch = next(iter(test_loader))\n",
    "xb, yb = batch\n",
    "xb = xb.to(device)\n",
    "with torch.no_grad():\n",
    "    probs = torch.sigmoid(model(xb)).cpu().view(-1)\n",
    "preds = (probs > THRESHOLD).int()\n",
    "\n",
    "for i in range(min(4, len(xb))):\n",
    "    patch = xb.cpu()[i]\n",
    "    # Create a quick RGB composite from bands [B4,B3,B2]\n",
    "    rgb = patch[[2,1,0]].numpy().transpose(1,2,0)\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow((rgb - rgb.min()) / (rgb.max() - rgb.min()))\n",
    "    plt.title(f\"True: {int(yb[i].item())}, Pred: {int(preds[i].item())}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d860df1",
   "metadata": {},
   "source": [
    "## Plot predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41026a72",
   "metadata": {},
   "source": [
    "#### 1. Parameter Definitions\n",
    "\n",
    "- Compute how many degrees correspond to half a patch\n",
    "\n",
    "- Define which bands (and indices) to sample\n",
    "\n",
    "- Parameters controlling image quality and the visual “fade” effect when plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 128\n",
    "scale_patch = 10\n",
    "half_m = (patch_size // 2) * scale_patch\n",
    "meters_per_deg = 111320.0\n",
    "half_deg  = half_m / meters_per_deg\n",
    "\n",
    "bands = ['B2','B3','B4','B8','NDTI','NDWI']\n",
    "window_days = 3\n",
    "cloud_thresh = 50\n",
    "map_scale = 30\n",
    "num_rings = 20\n",
    "max_alpha = 0.7\n",
    "r_pixels = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a08cf",
   "metadata": {},
   "source": [
    "#### 2. Build a List of Dates\n",
    "\n",
    "Creates a daily list of strings from date1 to date2, to loop over and fetch composites for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa46fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2025,5,1)\n",
    "end_date   = datetime(2025,5,20)\n",
    "dates_list = []\n",
    "d = start_date\n",
    "while d <= end_date:\n",
    "    dates_list.append(d.strftime('%Y-%m-%d'))\n",
    "    d += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37dc13",
   "metadata": {},
   "source": [
    "#### 3. Load the Trained Model\n",
    "\n",
    "Loads your previously‐trained CNN onto CPU/GPU, ready for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3281b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = PatchClassifier(in_channels=6).to(device)\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ca06f",
   "metadata": {},
   "source": [
    "#### 4. Load In-Situ Station Coordinates\n",
    "\n",
    "Reads the INVEMAR CSV, filters to just the four station names, and extracts unique lat/lon pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inv = pd.read_csv(\n",
    "    './data/icam.csv',\n",
    "    parse_dates=['muestreo'],\n",
    "    dayfirst=True\n",
    ")\n",
    "stations = ['F. Costa Verde','F. La Barra','F. Sevilla','F. Palma Sola']\n",
    "df_coords = (\n",
    "    df_inv[['latitud','longitud','estacion']]\n",
    "      .dropna()\n",
    "      .query('estacion in @stations')\n",
    "      .drop_duplicates(['latitud','longitud'])\n",
    ")\n",
    "lats = df_coords['latitud'].values\n",
    "lons = df_coords['longitud'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23ea11",
   "metadata": {},
   "source": [
    "#### 5. Define the Full‐Region Bounding Box\n",
    "\n",
    "Expands that station envelope by one patch radius to cover the entire area of interest in a single rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6039881",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lon = lons.min() - half_deg\n",
    "max_lon = lons.max() + half_deg\n",
    "min_lat = lats.min() - half_deg\n",
    "max_lat = lats.max() + half_deg\n",
    "region = ee.Geometry.Rectangle([min_lon, min_lat, max_lon, max_lat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae0665",
   "metadata": {},
   "source": [
    "#### 6. Loop Over Dates & Fetch Composites\n",
    "\n",
    "1. Filter Sentinel-2 by region, date window, and cloud cover\n",
    "\n",
    "2. Compute a median composite and two indices (NDTI, NDWI)\n",
    "\n",
    "3. Reproject to map_scale for a manageable full-region array\n",
    "\n",
    "4. Sample into a NumPy dict and assemble a normalized RGB image for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad77c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_str in dates_list[:3]:\n",
    "    date = pd.to_datetime(date_str, dayfirst=True)\n",
    "    start = (date - timedelta(days=window_days)).strftime('%Y-%m-%d')\n",
    "    end   = (date + timedelta(days=window_days)).strftime('%Y-%m-%d')\n",
    "\n",
    "    coll = (\n",
    "        ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "          .filterBounds(region)\n",
    "          .filterDate(start, end)\n",
    "          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', cloud_thresh))\n",
    "          .select(['B2','B3','B4','B8'])\n",
    "    )\n",
    "    if coll.size().getInfo() == 0:\n",
    "        continue\n",
    "\n",
    "    # build a median composite + compute NDTI & NDWI\n",
    "    base = coll.median()\n",
    "    ndti = base.normalizedDifference(['B4','B3']).rename('NDTI')\n",
    "    ndwi = base.normalizedDifference(['B3','B8']).rename('NDWI')\n",
    "    comp = base.addBands([ndti, ndwi]).unmask(0)\n",
    "\n",
    "    # downsample for full-region display\n",
    "    coarse = comp.reproject(comp.projection().atScale(map_scale))\n",
    "    info = coarse.sampleRectangle(region=region, defaultValue=0).toDictionary().getInfo()\n",
    "\n",
    "    # build a normalized RGB background\n",
    "    R = np.array(info['B4']); G = np.array(info['B3']); B = np.array(info['B2'])\n",
    "    rgb = np.stack([R, G, B], axis=2)\n",
    "    rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5721a165",
   "metadata": {},
   "source": [
    "#### 7. Compute Predictions at Station Locations\n",
    "\n",
    "- For each station coordinate, extract the same 128×128 patch (with the two indices) and normalize\n",
    "\n",
    "- Run it through the CNN to get a contamination probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79813356",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for lat, lon in zip(lats, lons):\n",
    "    box = ee.Geometry.Rectangle([\n",
    "        lon-half_deg, lat-half_deg,\n",
    "        lon+half_deg, lat+half_deg\n",
    "    ])\n",
    "    patch_img = (\n",
    "        coll.median()\n",
    "            .addBands([ndti, ndwi])\n",
    "            .unmask(0)\n",
    "            .reproject(comp.projection().atScale(scale_patch))\n",
    "    )\n",
    "    d = patch_img.sampleRectangle(region=box, defaultValue=0).toDictionary().getInfo()\n",
    "    arr = np.stack([np.array(d.get(b, np.zeros((patch_size, patch_size)))) for b in bands], axis=0)\n",
    "    arr = (arr - arr.mean(axis=(1,2), keepdims=True)) / (arr.std(axis=(1,2), keepdims=True) + 1e-8)\n",
    "    tensor = torch.from_numpy(arr).unsqueeze(0).to(device).float()\n",
    "    with torch.no_grad():\n",
    "        prob = torch.sigmoid(model(tensor)).cpu().item()\n",
    "    preds.append(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10de86f",
   "metadata": {},
   "source": [
    "#### 8. Convert to Pixel Coordinates\n",
    "\n",
    "Maps longitude/latitude to pixel x/y in the downsampled background image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3989752",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = rgb_norm.shape[:2]\n",
    "x_pix = [(lon-min_lon)/(max_lon-min_lon)*w for lon in lons]\n",
    "y_pix = [(max_lat-lat)/(max_lat-min_lat)*h for lat in lats]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de44fe",
   "metadata": {},
   "source": [
    "#### 9. Plot the Background + fading Circles\n",
    "\n",
    "1. Display the true-color Sentinel-2 background\n",
    "\n",
    "2. Overlay at each station a stack of transparent circles whose opacity fades outward, colored by the predicted probability\n",
    "\n",
    "3. Add a vertical colorbar to interpret the heatmap of contamination probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba964ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.imshow(rgb_norm)\n",
    "ax.set_title(f\"Contamination Map & GEE Image for {date.date()}\", y=1.03)\n",
    "ax.axis('off')\n",
    "\n",
    "cmap = plt.get_cmap('coolwarm')\n",
    "for x0, y0, p in zip(x_pix, y_pix, preds):\n",
    "    if np.isnan(p): \n",
    "        continue\n",
    "    color = cmap(p)\n",
    "\n",
    "    for i in reversed(range(1, num_rings+1)):\n",
    "        radius = r_pixels * (i / num_rings)\n",
    "        alpha  = (1 - (i / num_rings)) * max_alpha\n",
    "        circ = mpatches.Circle(\n",
    "            (x0, y0), radius=radius,\n",
    "            facecolor=color, edgecolor=None,\n",
    "            alpha=alpha, linewidth=0,\n",
    "            zorder=2\n",
    "        )\n",
    "        ax.add_patch(circ)\n",
    "\n",
    "sm = ScalarMappable(cmap='coolwarm', norm=plt.Normalize(vmin=0, vmax=1))\n",
    "sm.set_array([])\n",
    "plt.colorbar(sm, ax=ax, fraction=0.046, pad=0.04, label='Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669bde1",
   "metadata": {},
   "source": [
    "## OTRO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae78b80",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.3.1",
   "language": "python",
   "name": "pytorch-2.3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
